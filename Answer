1. Major changes made in Hadoop 2.x :-
                   Two of the most important advances in Hadoop 2 are the introduction of HDFS federation and the resource manager YARN.
       HDFS federation brings important measures of scalability and reliability to Hadoop. YARN, the other major advance in Hadoop 2,
       brings significant performance improvements for some applications, supports additional processing models, and implements a more
       flexible execution engine. YARN is a resource manager that was created by separating the processing engine and resource management
       capabilities of MapReduce as it was implemented in Hadoop 1. YARN is often called the operating system of Hadoop because it is
       responsible for managing and monitoring workloads, maintaining a multi-tenant environment, implementing security controls, and
       managing high availability features of Hadoop. Like an operating system on a server, YARN is designed to allow multiple, diverse
       user applications to run on a multi-tenant platform.
       In Hadoop 1, users had the option of writing MapReduce programs in Java, in 
       Python, Ruby or other scripting languages using streaming, or using Pig, a data transformation language. Regardless of which method
       was used, all fundamentally relied on the MapReduce processing model to run. YARN supports multiple processing models in addition
       to MapReduce. One of the most significant benefits of this is that we are no longer limited to working the often I/O intensive,
       high latency MapReduce framework. This advance means Hadoop users should be familiar with the pros and cons of the new processing
       models and understand when to apply them to particular use cases.
        
        There were also many important tools introduced, for example :
         1.Hive 
         2.Tez
         3.Spark
         
2. Differences between MapReduce 1 and MapReduce 2/YARN :-
  
        MapReduce 1 :
                      In a typical Hadoop cluster, racks are interconnected via core switches. Core switches should connect to top-of-rack
           switches Enterprises using Hadoop should consider using 10GbE, bonded Ethernet and redundant top-of-rack switches to mitigate
           risk in the event of failure. A file is broken into 64MB chunks by default and distributed across Data Nodes. Each chunk has a 
           default replication factor of 3, meaning there will be 3 copies of the data at any given time. Hadoop is “Rack Aware” and HDFS
           has replicated chunks on nodes on different racks. JobTracker assign tasks to nodes closest to the data depending on the
           location of nodes and helps the NameNode determine the ‘closest’ chunk to a client during reads. The administrator supplies a 
           script which tells Hadoop which rack the node is in, for example: /enterprisedatacenter/rack2.
           
           Limitations of MapReduce 1 :
                      Hadoop can scale up to 4,000 nodes. When it exceeds that limit, it raises unpredictable behavior such as cascading
                failures and serious deterioration of overall cluster. Another issue being multi-tenancy – it is impossible to 
                run other frameworks than MapReduce 1.0 on a Hadoop cluster.
           
        MapReduce 2 / YARN :
                       MapReduce 2.0 has two components – YARN that has cluster resource management capabilities and MapReduce.
               In MapReduce 2.0, the JobTracker is divided into three services:

                    i) ResourceManager, a persistent YARN service that receives and runs applications on the cluster.  A MapReduce job 
                 is an application.
                   ii) JobHistoryServer, to provide information about completed jobs
                  iii) Application Master, to manage each MapReduce job and is terminated when the job completes.
                  
               Also, the TaskTracker has been replaced with the NodeManager, a YARN service that manages resources and deployment on a 
             node. NodeManager is responsible for launching containers that could either be a map or reduce task.
               This new architecture breaks JobTracker model by allowing a new ResourceManager to manage resource usage across 
             applications, with ApplicationMasters taking the responsibility of managing the execution of jobs. This change removes a 
             bottleneck and let Hadoop clusters scale up to larger configurations than 4000 nodes. This architecture also allows
             simultaneous execution of a variety of programming models such as graph processing, iterative processing, machine learning,
             and general cluster computing, including the traditional MapReduce.

